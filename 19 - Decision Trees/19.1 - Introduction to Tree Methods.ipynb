{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entropy and Information Gain are the Mathematical Methods of choosing the best split\n",
    "\n",
    "### Random Forests\n",
    "- To improve performance, we can use many trees with a random sample of features chosen as the split\n",
    "- A new random sample of features is chosen for every single tree at every single split\n",
    "- For classification, m is typically chosen to be the square root of p total no. of features\n",
    "- Bagging - bootstrap the data plus using the aggregate to make a decision\n",
    "- Out-of-bag dataset - not chosen in bootstrapping\n",
    "- Out-of-bag error - proportion of Out-of-bag samples that were incorrectly classified\n",
    "\n",
    "### What's the point of using Random Forests?\n",
    "- Suppose there is one very strong feature in the dataset. When using \"bagged\" trees, most of the trees will use that feature as the top split, resulting in an ensemble of similar trees that are highly correlated\n",
    "- Averaging highly correlated quantities does not significantly reduce variance\n",
    "- By randomly leaving out candidate features from each split, Random Forests \"decorrelates\" the trees, such that the averaging process can reduce the variance of the resulting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
